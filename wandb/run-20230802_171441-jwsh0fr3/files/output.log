Loading and Processing Data...
Data MIMIC successfully loaded for features ['DBP', 'ESI', 'HR', 'RR', 'SBP', 'SPO2', 'TEMP', 'age', 'gender'] and outcomes ['Death', 'Discharge', 'ICU', 'Ward'].
(X, y) shape: (8328, 10, 9), (8328, 4)
Outcome Distribution: Death          26
Discharge    2860
ICU          1321
Ward         4121
dtype: int64
Loading and Training Model for fold 1...
 Logging experiments in exps/DirVRNN/Run_17_2023-08-02_17-14-47/fold_1
ERROR: NaNs in estimated representations
ERROR: NaNs in estimated representations
> /home/ball4537/PycharmProjects/VarPhenClustering/src/models/DirVRNN.py(333)forward()
-> for inner_t_idx, outer_t_idx in enumerate(range(_lower_t_idx, _upper_t_idx)):
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
LSTM_Dec_v1(
  (lstm_cell): LSTMCell(18, 128)
  (fc_out): Linear(in_features=128, out_features=18, bias=True)
)
*** AttributeError: 'LSTM_Dec_v1' object has no attribute 'weights'
*** NameError: name 'model' is not defined
*** AttributeError: 'LSTMCell' object has no attribute 'weight'
*** AttributeError: 'LSTMCell' object has no attribute 'layer_0'
*** NameError: name '__dir__' is not defined
<bound method Module.__dir__ of LSTM_Dec_v1(
  (lstm_cell): LSTMCell(18, 128)
  (fc_out): Linear(in_features=128, out_features=18, bias=True)
)>
['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'double', 'dropout', 'dump_patches', 'eval', 'extra_repr', 'fc_out', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'h_dim', 'half', 'i_dim', 'ipu', 'load_state_dict', 'lstm_cell', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'o_dim', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'seq_len', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']
['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'bias_hh', 'bias_ih', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'hidden_size', 'input_size', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_parameters', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight_hh', 'weight_ih', 'xpu', 'zero_grad']
Parameter containing:
tensor([[ 0.0619, -0.0577, -0.0838,  ...,  0.0336,  0.0715, -0.0848],
        [ 0.0299, -0.0295, -0.0807,  ..., -0.0474,  0.0425,  0.0376],
        [-0.0088,  0.1070,  0.0687,  ...,  0.0986, -0.0224,  0.0210],
        ...,
        [-0.0231, -0.0701,  0.0561,  ...,  0.0350,  0.0638, -0.0346],
        [ 0.0068,  0.0510, -0.0409,  ...,  0.0214,  0.0758, -0.0636],
        [ 0.0782,  0.0292, -0.0728,  ..., -0.0181,  0.0376,  0.0685]],
       requires_grad=True)
Parameter containing:
tensor([[ 0.0862, -0.0704, -0.0266,  ...,  0.0495,  0.0745, -0.0607],
        [-0.0420, -0.0675,  0.0716,  ..., -0.0093,  0.0382,  0.0690],
        [-0.0640, -0.0744, -0.0807,  ..., -0.0615,  0.0525, -0.0505],
        ...,
        [ 0.0641, -0.0042, -0.0441,  ...,  0.0408, -0.0280,  0.0799],
        [-0.0203, -0.0393,  0.0060,  ..., -0.0215, -0.0738,  0.0713],
        [ 0.0760, -0.0514,  0.0157,  ..., -0.0076, -0.0212,  0.0154]],
       requires_grad=True)
*** AssertionError: LSTMCell: Expected input to be 1-D or 2-D but received 3-D tensor
*** SyntaxError: '(' was never closed
*** RuntimeError: input has inconsistent input_size: got 9 expected 18
*** RuntimeError: shape '[-1, 18]' is invalid for input of size 32
(tensor([[-0.0191,  0.0115, -0.0340,  ..., -0.0262,  0.0431, -0.0019],
        [-0.0110, -0.0083, -0.0167,  ..., -0.0310,  0.0197,  0.0138],
        [-0.0260,  0.0057, -0.0178,  ..., -0.0289,  0.0365,  0.0030],
        ...,
        [-0.0169,  0.0227, -0.0110,  ..., -0.0339,  0.0399, -0.0088],
        [-0.0099, -0.0031, -0.0011,  ..., -0.0500,  0.0388,  0.0158],
        [-0.0063, -0.0044, -0.0103,  ..., -0.0253,  0.0182, -0.0023]],
       grad_fn=<MulBackward0>), tensor([[-0.0387,  0.0233, -0.0653,  ..., -0.0546,  0.0901, -0.0038],
        [-0.0226, -0.0163, -0.0326,  ..., -0.0639,  0.0394,  0.0285],
        [-0.0515,  0.0114, -0.0339,  ..., -0.0600,  0.0766,  0.0060],
        ...,
        [-0.0342,  0.0467, -0.0213,  ..., -0.0720,  0.0833, -0.0184],
        [-0.0202, -0.0061, -0.0021,  ..., -0.1082,  0.0853,  0.0327],
        [-0.0130, -0.0090, -0.0208,  ..., -0.0539,  0.0356, -0.0048]],
       grad_fn=<AddBackward0>))
*** AttributeError: 'DirVRNN' object has no attribute 'fc_out'
Linear(in_features=128, out_features=18, bias=True)
['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'double', 'dropout', 'dump_patches', 'eval', 'extra_repr', 'fc_out', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'h_dim', 'half', 'i_dim', 'ipu', 'load_state_dict', 'lstm_cell', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'o_dim', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'seq_len', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']
MLP(
  (nn_layers): ModuleList(
    (0): Linear(in_features=128, out_features=50, bias=True)
    (1): Linear(in_features=50, out_features=50, bias=True)
    (2): Linear(in_features=50, out_features=6, bias=True)
  )
  (act_fn): ReLU()
  (dropout): Dropout(p=0.6, inplace=False)
)
*** RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x9 and 128x50)
torch.Size([32, 10, 9])
*** SyntaxError: '(' was never closed
*** SyntaxError: unmatched ')'
*** RuntimeError: mat1 and mat2 shapes cannot be multiplied (16x18 and 128x50)
*** RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x9 and 128x50)
*** TypeError: DirVRNN._encoder_pass() missing 1 required positional argument: 'x'
Documented commands (type help <topic>):
========================================
EOF    c          d        h         list      q        rv       undisplay
a      cl         debug    help      ll        quit     s        unt
alias  clear      disable  ignore    longlist  r        source   until
args   commands   display  interact  n         restart  step     up
b      condition  down     j         next      return   tbreak   w
break  cont       enable   jump      p         retval   u        whatis
bt     continue   exit     l         pp        run      unalias  where
Miscellaneous help topics:
==========================
exec  pdb
*** NameError: name 'z' is not defined
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)
tensor([[nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward0>)
*** NameError: name 'tes' is not defined
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<ReluBackward0>)
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<ReluBackward0>)
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0.0753, 0.0000, 0.5948, 0.0939, 0.5401, 0.9400, 0.5694, 0.4384, 0.0000],
        [0.0457, 0.0000, 0.4655, 0.0608, 0.3502, 1.0000, 0.5694, 0.6712, 1.0000],
        [0.1089, 0.0000, 0.2328, 0.0552, 0.5949, 0.9900, 0.5651, 0.5205, 1.0000],
        [0.0806, 0.0000, 0.3190, 0.0552, 0.5063, 0.9800, 0.5728, 0.6301, 1.0000],
        [0.1169, 0.0000, 0.3621, 0.0608, 0.6287, 0.9900, 0.5711, 0.6575, 0.0000],
        [0.0974, 0.0000, 0.6897, 0.0663, 0.5443, 0.9550, 0.5737, 0.5205, 1.0000],
        [0.0766, 0.0000, 0.3276, 0.0718, 0.7046, 0.9900, 0.5677, 0.6986, 0.0000],
        [0.0806, 0.5000, 0.4655, 0.0552, 0.7004, 0.9700, 0.5651, 0.8630, 0.0000],
        [0.0538, 0.5000, 0.2759, 0.0552, 0.4346, 0.9900, 0.5686, 0.3973, 1.0000],
        [0.0914, 0.5000, 0.5948, 0.0552, 0.3797, 0.9800, 0.5617, 0.0000, 1.0000],
        [0.0887, 0.5000, 0.3534, 0.0663, 0.4473, 1.0000, 0.5686, 0.4110, 0.0000],
        [0.0927, 0.5000, 0.3707, 0.0552, 0.4937, 0.9800, 0.5728, 0.0274, 0.0000],
        [0.0578, 0.5000, 0.1466, 0.0773, 0.6709, 1.0000, 0.5720, 0.9178, 0.0000],
        [0.0811, 0.0000, 0.4828, 0.1289, 0.5260, 0.9600, 0.5643, 0.8767, 0.0000],
        [0.1022, 0.5000, 0.7069, 0.0552, 0.5401, 1.0000, 0.5677, 0.0000, 0.0000],
        [0.0605, 0.0000, 0.0517, 0.0497, 0.4937, 0.9500, 0.5660, 0.7260, 1.0000],
        [0.1048, 0.0000, 0.5172, 0.0663, 0.4515, 0.9900, 0.5694, 0.5890, 1.0000],
        [0.0968, 0.0000, 0.1897, 0.0773, 0.5527, 0.9700, 0.5600, 1.0000, 0.0000],
        [0.0659, 0.0000, 0.3190, 0.0663, 0.5359, 0.9800, 0.5694, 0.9315, 0.0000],
        [0.0618, 0.5000, 0.1379, 0.0442, 0.3924, 1.0000, 0.5677, 0.6164, 1.0000],
        [0.0766, 0.0000, 0.5517, 0.0331, 0.5949, 0.9900, 0.5788, 0.6027, 0.0000],
        [0.0954, 0.0000, 0.3448, 0.0552, 0.6203, 1.0000, 0.5660, 0.6986, 1.0000],
        [0.1250, 0.0000, 0.6293, 0.0884, 0.6456, 0.9900, 0.5839, 0.5205, 1.0000],
        [0.0739, 0.5000, 0.3017, 0.0773, 0.4262, 1.0000, 0.5617, 0.0411, 0.0000],
        [0.0502, 0.0000, 0.7500, 0.1160, 0.3390, 0.9500, 0.5796, 0.4932, 1.0000],
        [0.0766, 0.5000, 0.5948, 0.0773, 0.5063, 0.9500, 0.5737, 0.7534, 1.0000],
        [0.0968, 0.5000, 0.5259, 0.0663, 0.4768, 1.0000, 0.5677, 0.2740, 0.0000],
        [0.0773, 0.5000, 0.4957, 0.0691, 0.4684, 0.9900, 0.5724, 0.4384, 1.0000],
        [0.0901, 0.5000, 0.2845, 0.0497, 0.6329, 1.0000, 0.5694, 0.5068, 0.0000],
        [0.0672, 0.0000, 0.3707, 0.0442, 0.4051, 0.9800, 0.5677, 0.7397, 0.0000],
        [0.1035, 0.0000, 0.3707, 0.0663, 0.5865, 0.9700, 0.5703, 0.0685, 1.0000],
        [0.1008, 0.5000, 0.6466, 0.0552, 0.5907, 0.9900, 0.5634, 0.0274, 1.0000]])
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)
MLP(
  (nn_layers): ModuleList(
    (0): Linear(in_features=9, out_features=50, bias=True)
    (1): Linear(in_features=50, out_features=50, bias=True)
    (2): Linear(in_features=50, out_features=64, bias=True)
  )
  (act_fn): ReLU()
  (dropout): Dropout(p=0.6, inplace=False)
)
['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_activation', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'act_fn', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'double', 'dropout', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'nn_layers', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']
ModuleList(
  (0): Linear(in_features=9, out_features=50, bias=True)
  (1): Linear(in_features=50, out_features=50, bias=True)
  (2): Linear(in_features=50, out_features=64, bias=True)
)
['T_destination', '__add__', '__annotations__', '__call__', '__class__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_abs_string_index', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'add_module', 'append', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extend', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'insert', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'pop', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']
Linear(in_features=9, out_features=50, bias=True)
['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'in_features', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_parameters', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad']
Parameter containing:
tensor([[    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.0675,  0.0108,  0.0305, -0.0243, -0.0912,  0.0598,  0.3303, -0.2331,
         -0.1563],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.1984, -0.1643,  0.1962, -0.0724,  0.2660, -0.2848, -0.2976,  0.0532,
         -0.1025],
        [-0.3190,  0.0222, -0.1456,  0.3323, -0.3281,  0.0326, -0.1245, -0.2008,
         -0.3072],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [ 0.2239, -0.1617, -0.1948, -0.2691, -0.1240, -0.0435, -0.0024,  0.0779,
          0.1915],
        [-0.3443, -0.1887, -0.0239, -0.2516, -0.2916,  0.1380, -0.3053, -0.3419,
          0.1058],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.1135,  0.3264,  0.2018,  0.1906, -0.2378, -0.1306, -0.3082,  0.1963,
         -0.0260],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.0333,  0.2955, -0.1810,  0.0399,  0.1751, -0.2454, -0.1566, -0.1666,
          0.1718],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.0827, -0.1523,  0.0098, -0.2079, -0.2294, -0.1418, -0.2967,  0.0680,
         -0.2208],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.2202,  0.1749, -0.0340, -0.0988, -0.1013,  0.1196, -0.1986, -0.0653,
         -0.2409],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [ 0.1650, -0.1021, -0.0769, -0.1768, -0.0674, -0.2163, -0.0777, -0.1853,
         -0.2353],
        [-0.0665, -0.0751, -0.0462, -0.3277, -0.2836,  0.1630, -0.2239, -0.2554,
          0.2891],
        [-0.2632, -0.2812,  0.0524, -0.0457, -0.1006, -0.2372, -0.0456, -0.3062,
         -0.2928],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [ 0.3031, -0.0812, -0.2599, -0.3260, -0.3082, -0.1712, -0.0884,  0.1790,
         -0.0162],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.2707, -0.2969, -0.0766, -0.3148,  0.1449, -0.2364, -0.2087, -0.3425,
         -0.0104]], requires_grad=True)
Parameter containing:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)
Parameter containing:
tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
        ...,
        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
        [-0.1413, -0.0812, -0.0568,  ...,  0.0636, -0.1105, -0.0586],
        [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],
       requires_grad=True)
Parameter containing:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)
Parameter containing:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)
Parameter containing:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)
MLP(
  (nn_layers): ModuleList(
    (0): Linear(in_features=64, out_features=50, bias=True)
    (1): Linear(in_features=50, out_features=50, bias=True)
    (2): Linear(in_features=50, out_features=4, bias=True)
  )
  (act_fn): ReLU()
  (dropout): Dropout(p=0.6, inplace=False)
)
Parameter containing:
tensor([[ 0.1207, -0.0024, -0.0147,  ..., -0.0209, -0.0281, -0.0877],
        [ 0.0680, -0.0514,  0.0839,  ..., -0.0813,  0.0153,  0.0960],
        [-0.0286,  0.0115, -0.0268,  ..., -0.0925, -0.1029,  0.1042],
        ...,
        [ 0.0552,  0.0299, -0.0420,  ..., -0.0849, -0.0931, -0.0662],
        [ 0.0476, -0.0375,  0.0848,  ..., -0.0156, -0.0099, -0.0249],
        [ 0.1078, -0.0593, -0.0871,  ..., -0.0364, -0.0259, -0.0912]],
       requires_grad=True)
Parameter containing:
tensor([[    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.0675,  0.0108,  0.0305, -0.0243, -0.0912,  0.0598,  0.3303, -0.2331,
         -0.1563],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.1984, -0.1643,  0.1962, -0.0724,  0.2660, -0.2848, -0.2976,  0.0532,
         -0.1025],
        [-0.3190,  0.0222, -0.1456,  0.3323, -0.3281,  0.0326, -0.1245, -0.2008,
         -0.3072],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [ 0.2239, -0.1617, -0.1948, -0.2691, -0.1240, -0.0435, -0.0024,  0.0779,
          0.1915],
        [-0.3443, -0.1887, -0.0239, -0.2516, -0.2916,  0.1380, -0.3053, -0.3419,
          0.1058],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.1135,  0.3264,  0.2018,  0.1906, -0.2378, -0.1306, -0.3082,  0.1963,
         -0.0260],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.0333,  0.2955, -0.1810,  0.0399,  0.1751, -0.2454, -0.1566, -0.1666,
          0.1718],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.0827, -0.1523,  0.0098, -0.2079, -0.2294, -0.1418, -0.2967,  0.0680,
         -0.2208],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.2202,  0.1749, -0.0340, -0.0988, -0.1013,  0.1196, -0.1986, -0.0653,
         -0.2409],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [ 0.1650, -0.1021, -0.0769, -0.1768, -0.0674, -0.2163, -0.0777, -0.1853,
         -0.2353],
        [-0.0665, -0.0751, -0.0462, -0.3277, -0.2836,  0.1630, -0.2239, -0.2554,
          0.2891],
        [-0.2632, -0.2812,  0.0524, -0.0457, -0.1006, -0.2372, -0.0456, -0.3062,
         -0.2928],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [ 0.3031, -0.0812, -0.2599, -0.3260, -0.3082, -0.1712, -0.0884,  0.1790,
         -0.0162],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan],
        [-0.2707, -0.2969, -0.0766, -0.3148,  0.1449, -0.2364, -0.2087, -0.3425,
         -0.0104]], requires_grad=True)
torch.Size([50, 9])
*** AttributeError: 'DirVRNN' object has no attribute 'batch_size'
