{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Run Single Experiments for Traditional Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do not change the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required packages\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from src.models.ESI.model import ESI\n",
    "from src.dataloading.TrainingDataLoader import TrainingDataLoader\n",
    "\n",
    "from src.metrics.summarize import summary_binary_metrics, print_avg_metrics_paper\n",
    "import src.logging.logger_utils as logger_utils\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "#### LOAD CONFIGURATIONS\n",
    "with open(\"src/models/ESI/run_config.json\", \"r\") as f:\n",
    "    args = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8328it [00:40, 206.53it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### LOAD DATA\n",
    "data_loader = TrainingDataLoader(\n",
    "    data_dir=args[\"data_dir\"],\n",
    "    time_window=args[\"time_window\"],\n",
    "    feat_subset=args[\"feat_subset\"],\n",
    "    train_test_ratio=args[\"train_test_ratio\"],\n",
    "    train_val_ratio=args[\"train_val_ratio\"],\n",
    "    seed=args[\"seed\"],\n",
    "    normalize=args[\"normalize\"],\n",
    "    num_folds=args[\"num_folds\"]\n",
    ")\n",
    "data_characteristics = data_loader._get_data_characteristics()\n",
    "\n",
    "# Unpack\n",
    "input_shape = data_characteristics[\"num_samples\"], data_characteristics[\"num_timestamps\"], data_characteristics[\"num_features\"]\n",
    "output_dim = data_characteristics[\"num_outcomes\"]\n",
    "ESI_idx = data_characteristics[\"features\"].index(\"ESI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### TRAIN MODEL\n",
    "model = ESI(input_shape=input_shape, output_dim=output_dim, ESI_idx=ESI_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training ESI model...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This does nothing for ESI\n",
    "X_train, y_train = data_loader.get_train_X_y(fold=0)\n",
    "model.train(train_data=(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set and Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### EVALUATE MODEL\n",
    "\n",
    "# Performance on Test data\n",
    "X_test, y_test = data_loader.get_test_X_y(fold=0, mode=\"test\")\n",
    "y_pred = model.predict(X_test, y=y_test)\n",
    "y_pred = y_pred / np.max(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare labels and output dic\n",
    "output_dic = {}\n",
    "labels_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# Compute Metrics and visualize\n",
    "for out_idx in range(output_dim):\n",
    "\n",
    "    # Iterate over each outcome\n",
    "    has_outcome = labels_test == out_idx\n",
    "    pred_score = y_pred\n",
    "\n",
    "    # Compute scores\n",
    "    output_dic[out_idx] = summary_binary_metrics(labels_true=has_outcome, scores_pred=pred_score)\n",
    "\n",
    "# Aggreggate all metrics over the keys in output_dic\n",
    "metric_keys = output_dic[0].keys()\n",
    "metrics_dict = {\n",
    "    key: [] for key in metric_keys\n",
    "}\n",
    "for out_idx in range(output_dim):\n",
    "    for key in metric_keys:\n",
    "        metrics_dict[key].append(output_dic[out_idx][key])\n",
    "\n",
    "# Convert for compatibility\n",
    "metrics_dict[\"macro_f1_score\"] = metrics_dict[\"f1_score\"]\n",
    "metrics_dict[\"ovr_auroc\"] = metrics_dict[\"auroc\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Results and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:13:51.382894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log Model, Results and Visualizations\n",
    "cur_time_as_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "test_dir = f\"results/ESI/{cur_time_as_str}/\"\n",
    "\n",
    "# Save outputs into data objects and run information\n",
    "data_objects = {\n",
    "    \"y_pred\": y_pred,\n",
    "    \"labels_test\": labels_test,\n",
    "    \"X\": (X_train, X_test),\n",
    "    \"y\": (y_train, y_test),\n",
    "}\n",
    "\n",
    "run_info = {\n",
    "    \"data_characteristics\": data_characteristics,\n",
    "    \"args\": args,\n",
    "    \"metrics\": metrics_dict,\n",
    "}\n",
    "\n",
    "model.log_model(save_dir=test_dir, objects_to_log=data_objects, run_info=run_info)\n",
    "print(\"Time taken: \", datetime.now() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.003001200480192077,\n",
       "  0.3433373349339736,\n",
       "  0.15876350540216086,\n",
       "  0.49489795918367346],\n",
       " 'f1_score': [0.005984440395116325,\n",
       "  0.5111706843068743,\n",
       "  0.2740222716560729,\n",
       "  0.6621160365238007],\n",
       " 'precision': [0.00300120048018307,\n",
       "  0.34333733493294316,\n",
       "  0.15876350540168438,\n",
       "  0.4948979591821882],\n",
       " 'recall': [0.9999999989999999,\n",
       "  0.9999999999912589,\n",
       "  0.9999999999810963,\n",
       "  0.9999999999939357],\n",
       " 'auroc': [0.39695966, 0.56600946, 0.3568213, 0.51819646],\n",
       " 'auprc': [0.0026974764, 0.37969515, 0.14325953, 0.50437856],\n",
       " 'confusion_matrix': [array([[  10, 3322],\n",
       "         [   0,    0]]),\n",
       "  array([[1144, 2188],\n",
       "         [   0,    0]]),\n",
       "  array([[ 529, 2803],\n",
       "         [   0,    0]]),\n",
       "  array([[1649, 1683],\n",
       "         [   0,    0]])],\n",
       " 'true_false_pos_neg': [(10, 0, 3322, 0),\n",
       "  (1144, 0, 2188, 0),\n",
       "  (529, 0, 2803, 0),\n",
       "  (1649, 0, 1683, 0)],\n",
       " 'lachiche': [{}, {}, {}, {}],\n",
       " 'lachiche_threshold': [None, None, None, None],\n",
       " 'macro_f1_score': [0.005984440395116325,\n",
       "  0.5111706843068743,\n",
       "  0.2740222716560729,\n",
       "  0.6621160365238007],\n",
       " 'ovr_auroc': [0.39695966, 0.56600946, 0.3568213, 0.51819646]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log to CSV Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro_f1_score: 0.363\n",
      "precision: 0.250\n",
      "recall: 1.000\n",
      "ovr_auroc: 0.459\n",
      "silhouette: N/A\n",
      "davies_bouldin: N/A\n",
      "calinski_harabasz: N/A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================== CSV LOGGING\n",
    "csv_path = \"results/ESI/tracker.csv\"\n",
    "\n",
    "params_header = [key for key in args.keys() if key not in [\"model_params\"]]\n",
    "metrics_header = [\"F1\", \"Precision\", \"Recall\", \"Auroc\", \"SIL\", \"DBI\", \"VRI\"]\n",
    "logger_utils.make_csv_if_not_exists(csv_path, params_header + metrics_header)\n",
    "\n",
    "# Append Row\n",
    "metrics_to_print = print_avg_metrics_paper(metrics_dict)\n",
    "row_append = *[args[key] for key in params_header], *metrics_to_print\n",
    "logger_utils.write_csv_row(csv_path, row_append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro_f1_score: 0.001\n",
      "precision: 0.001\n",
      "recall: 0.250\n",
      "ovr_auroc: 0.459\n"
     ]
    }
   ],
   "source": [
    "for name in [\"macro_f1_score\", \"precision\", \"recall\", \"ovr_auroc\"]:\n",
    "    print(f\"{name}: {np.mean(metrics_dict[name]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10 1144  529 1649]\n",
      " [   0    0    0    0]\n",
      " [   0    0    0    0]\n",
      " [   0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics_dict[\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
